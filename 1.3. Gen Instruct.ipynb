{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "639c35f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ldap-users-2/dwipraseetyo-a/Project/Qwen2.5-Omni\n"
     ]
    }
   ],
   "source": [
    "%cd /home/is/dwipraseetyo-a/NAS_HAI/Project/Qwen2.5-Omni\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "import os, librosa, random, pickle, pydicom, requests, torch, re\n",
    "from pydicom.datadict import keyword_for_tag\n",
    "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import commons, const_variable\n",
    "random.seed(42)\n",
    "\n",
    "DATA_PATH = \"/home/is/dwipraseetyo-a/NAS_HAI/Datasets/cidrz\"\n",
    "\n",
    "# system_prompt = '''\n",
    "# You are an advanced medical assistant AI specialized in analyzing and diagnosing clinical conditions, capable of perceiving auditory and visual inputs. \n",
    "# You can interpret and reason over various medical inputs, including auditory inputs, visual inputs, and patient symptoms, individually or in combination, depending on what is provided. \n",
    "# Your task is to analyze the given input, and give a possible diagnosis. \n",
    "# '''\n",
    "\n",
    "# system_prompt = (\n",
    "#     \"A conversation between User and Advanced medical assistant specialized in analyzing and diagnosing clinical conditions. and the Assistant determines whether the case is Positive or Negative. The assistant \"\n",
    "#     \"first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning \"\n",
    "#     \"process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., \"\n",
    "#     \"<think> reasoning process here </think><answer> answer here </answer>\"\n",
    "# )\n",
    "\n",
    "system_prompt = '''\n",
    "You are an advanced medical assistant AI specialized in analyzing and diagnosing clinical conditions, capable of perceiving auditory and visual inputs. \n",
    "You can interpret and reason over various medical inputs, including auditory inputs, visual inputs, and patient symptoms, individually or in combination, depending on what is provided. \n",
    "Your task is to analyze the given input, explain your reasoning, and give a possible diagnosis. Answer are enclosed within <answer> </answer> tags, respectively, i.e., <answer> answer here </answer> .\n",
    "Always respond in the following format:\n",
    "\n",
    "## ‚ö†Ô∏è Points to Review and Disclaimer\n",
    "<If no auditory or visual input is provided>\n",
    "\n",
    "## üß† Overview\n",
    "<answer> <Positive or Negative Diagnosis> </answer>\n",
    "\n",
    "## üìã Observations\n",
    "**Chest X-ray:**\n",
    "<Your explanation based on the relevant visual input>\"\n",
    "\n",
    "**Symptoms:**\n",
    "<Your explanation based on the input symptoms>\"\n",
    "\n",
    "**Audio:**\n",
    "<Your explanation based on the input audio>\"\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e356703",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3057582/3122668694.py:3: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_llm_symptoms = ( pd.read_csv(f\"datas/reasoning/symptoms/gpt-4o-mini_symptoms.csv.{split}\").groupby('barcode', group_keys=False).apply(lambda x: x.sample(1), include_groups=True).reset_index(drop=True) )\n",
      "/tmp/ipykernel_3057582/3122668694.py:4: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_llm_images = ( pd.read_csv(f\"datas/reasoning/xray/medgemma_xray_formatted.csv.{split}\").groupby('path_file_image', group_keys=False).apply(lambda x: x.sample(1), include_groups=True).reset_index(drop=True) )\n"
     ]
    }
   ],
   "source": [
    "for split in ['train', 'dev']:\n",
    "    df = pd.read_csv(f\"{DATA_PATH}/metadata_cut_processed.csv.{split}\")\n",
    "    df_llm_symptoms = ( pd.read_csv(f\"datas/reasoning/symptoms/gpt-4o-mini_symptoms.csv.{split}\").groupby('barcode', group_keys=False).apply(lambda x: x.sample(1), include_groups=True).reset_index(drop=True) ) \n",
    "    df_llm_images = ( pd.read_csv(f\"datas/reasoning/xray/medgemma_xray_formatted.csv.{split}\").groupby('path_file_image', group_keys=False).apply(lambda x: x.sample(1), include_groups=True).reset_index(drop=True) )\n",
    "    df = pd.merge(df, df_llm_symptoms, on='barcode', how='left')\n",
    "    df = pd.merge(df, df_llm_images, on='path_file_image', how='left')\n",
    "    df = df.rename(columns={'coughdur': 'cough_duration', 'ngtsweats': 'night_sweets', 'weightloss': 'weight_loss', 'body_wt': 'body_weight'})\n",
    "\n",
    "    case_info = df['ground_truth_tb'].value_counts().reset_index()\n",
    "    max_count = case_info['count'][0]\n",
    "    minor_count = case_info['count'][1]\n",
    "    ratio = int(max_count // minor_count)\n",
    "\n",
    "    #df_temp = pd.DataFrame(columns=['barcode', 'question', 'answer', 'tb_status', 'audio_path', 'image_path'])\n",
    "    instruct_array = []\n",
    "    for now_row in tqdm(df.itertuples(), desc=f\"Processing {split}\", total=len(df)):\n",
    "        row_dict = now_row._asdict()\n",
    "        now_audiopath = \"/home/is/dwipraseetyo-a/NAS_HAI/Datasets/cidrz/\" +  now_row.path_file_audio\n",
    "        now_imgaepath = \"/home/is/dwipraseetyo-a/NAS_HAI/Datasets/cidrz/\" +  now_row.path_file_image\n",
    "        current_augment = ratio if now_row.ground_truth_tb == 1 else 1\n",
    "\n",
    "        for i in range(current_augment):\n",
    "            question = \"\"\n",
    "            array_df = [None, None]\n",
    "            last_sentence_question, modalities = commons.unique_modalities_generator(const_variable.prompt_templates, now_row.path_file_audio)\n",
    "            last_sentence_question += \". \"\n",
    "            #templates = const_variable.positive_templates if now_row.ground_truth_tb == 1 else const_variable.negative_templates\n",
    "            #answer = random.choice(templates)\n",
    "            answer = commons.generate_tb_response(modalities, now_row.llm_analyze_symptoms, now_row.llm_analyze_image, positive=(now_row.ground_truth_tb == 1))       \n",
    "            \n",
    "            array_df = [None, None]\n",
    "            modalities_tags = \"\"\n",
    "            if \"symptoms\" in modalities:\n",
    "                row_dict = now_row._asdict()\n",
    "                #selected_feats = random.sample(const_variable.columns_soundfeat, k=random.randint(3, len(const_variable.columns_soundfeat)))\n",
    "                symptom_descriptions = \", \".join(\n",
    "                    f\"{feat.replace('_', ' ')} is {row_dict[feat]}\"\n",
    "                    for feat in const_variable.columns_soundfeat #selected_feats\n",
    "                    if row_dict.get(feat) != \"Unknown\"\n",
    "                )\n",
    "                if symptom_descriptions:\n",
    "                    question += f\"The patient symptoms are {symptom_descriptions}.\"\n",
    "\n",
    "            if \"audio\" in modalities:\n",
    "                modalities_tags += \"<audio>\"\n",
    "                array_df[0] = now_audiopath\n",
    "\n",
    "            if \"xray\" in modalities:\n",
    "                modalities_tags += \"<image>\"\n",
    "                array_df[1] = now_imgaepath\n",
    "                xray_descriptions = \", \".join(\n",
    "                    f\"{feat.replace('_', ' ')} is {row_dict[feat]}\"\n",
    "                    for feat in const_variable.columns_imagefeat\n",
    "                    if row_dict.get(feat) != \"Unknown\"\n",
    "                )\n",
    "                if xray_descriptions:\n",
    "                    question += f\" The chest x-ray metadata are {xray_descriptions}.\"\n",
    "\n",
    "            question = question.strip()\n",
    "            question = question.rstrip(\",.\")\n",
    "            if not question.endswith(\".\"):\n",
    "                question += \". \"\n",
    "            \n",
    "            question = modalities_tags + \"\" + question\n",
    "            # temp_sentence_arry = last_sentence_question.split(\",\")\n",
    "            # if len(temp_sentence_arry) >= 2:\n",
    "            #     temp_sentence_arry[-2] += \" \" + modalities_tags\n",
    "            # question += \",\".join(temp_sentence_arry)\n",
    "            question += last_sentence_question\n",
    "\n",
    "            temp_instruct = {\"messages\": [\n",
    "                {\"role\": \"system\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": system_prompt}\n",
    "                    ],\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"text\", \"text\": question},\n",
    "                    ],\n",
    "                },\n",
    "                {\"role\": \"assistant\", \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": answer}]},\n",
    "            ]}\n",
    "            if array_df[0] != None:\n",
    "                temp_instruct[\"messages\"][1]['content'].append({\"type\": \"audio\", \"audio\": array_df[0]})\n",
    "            if array_df[1] != None:\n",
    "                temp_instruct[\"messages\"][1]['content'].append({\"type\": \"image\", \"image\": array_df[1]})\n",
    "            instruct_array.append(temp_instruct)            \n",
    "            #df_temp.loc[len(df_temp)] = [now_row.barcode, question, answer, now_row.ground_truth_tb, now_audiopath, now_imgaepath]\n",
    "    with open(f\"datas/instruct_sft_balance.pkl.{split}\", \"wb\") as f:\n",
    "        pickle.dump(instruct_array, f)\n",
    "    #pd.DataFrame(df_temp).to_csv(f'datas/grpo_balance.csv.{split}', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83eb0f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ceea336",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Negative Tuberculosis'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(r'<answer>(.*?)</answer>', \"<answer> Negative Tuberculosis </answer>\", flags=re.IGNORECASE).group(1).strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e87355f",
   "metadata": {},
   "source": [
    "# Publish To HG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8bc439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================\n",
      "| üì¶ Loading Dataset... |\n",
      "========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3526/3526 [00:31<00:00, 112.05it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 439/439 [00:03<00:00, 114.88it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3526/3526 [00:55<00:00, 63.95it/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 439/439 [00:04<00:00, 99.13it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "675633f99b2a4438922e08c4fbaf9641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaa000e4c14c4cee82c17fbef2ffd6fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be11a6ad1174f70918fa3f2bd696ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eeeb217519f444fa9c439aca43f482e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b54229136f6400ea401f653d16d092f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/arkiven4/cirdz-instruct/commit/8538285dc2f17ddce47fb960f49054e6c7d10419', commit_message='Upload dataset', commit_description='', oid='8538285dc2f17ddce47fb960f49054e6c7d10419', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/arkiven4/cirdz-instruct', endpoint='https://huggingface.co', repo_type='dataset', repo_id='arkiven4/cirdz-instruct'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "from dotenv import load_dotenv\n",
    "from datasets import Dataset, concatenate_datasets\n",
    "\n",
    "load_dotenv()\n",
    "login(token=os.getenv(\"HG_AUTH_KEY_WRITE\"))\n",
    "\n",
    "commons.pretty_status(\"üì¶ Loading Dataset...\")\n",
    "\n",
    "with open('datas/instruct_grpo_balance.pkl.train', 'rb') as f:\n",
    "    train_instruct = commons.load_image_PIL(pickle.load(f))\n",
    "\n",
    "with open('datas/instruct_grpo_balance.pkl.dev', 'rb') as f:\n",
    "    dev_instruct = commons.load_image_PIL(pickle.load(f))\n",
    "    \n",
    "train_dataset = Dataset.from_list(commons.grpo_build_datasets(train_instruct, None)) #\n",
    "val_dataset = Dataset.from_list(commons.grpo_build_datasets(dev_instruct, None))\n",
    "\n",
    "combined = concatenate_datasets([train_dataset, val_dataset])\n",
    "combined.push_to_hub(\"arkiven4/cirdz-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4e4ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{DATA_PATH}/metadata_cut_processed.csv.test\")\n",
    "df_llm_symptoms = ( pd.read_csv(f\"datas/reasoning/symptoms/o4-mini_symptoms.csv.test\").groupby('barcode', group_keys=False).apply(lambda x: x.sample(1), include_groups=True).reset_index(drop=True) ) \n",
    "df_llm_images = ( pd.read_csv(f\"datas/reasoning/xray/medgemma_xray_formatted.csv.test\").groupby('path_file_image', group_keys=False).apply(lambda x: x.sample(1), include_groups=True).reset_index(drop=True) )\n",
    "df = pd.merge(df, df_llm_symptoms, on='barcode', how='left')\n",
    "df = pd.merge(df, df_llm_images, on='path_file_image', how='left')\n",
    "df = df.rename(columns={'coughdur': 'cough_duration', 'ngtsweats': 'night_sweets', 'weightloss': 'weight_loss', 'body_wt': 'body_weight'})\n",
    "\n",
    "instruct_array_positive = []\n",
    "instruct_array_negative = []\n",
    "\n",
    "for now_row in tqdm(df.itertuples(), desc=f\"Processing Datasets\", total=len(df)):\n",
    "    row_dict = now_row._asdict()\n",
    "    now_audiopath = \"/home/is/dwipraseetyo-a/NAS_HAI/Datasets/cidrz/\" +  now_row.path_file_audio\n",
    "    now_imgaepath = \"/home/is/dwipraseetyo-a/NAS_HAI/Datasets/cidrz/\" +  now_row.path_file_image\n",
    "\n",
    "    modalities = [\"audio\", \"xray\", \"symptoms\"]\n",
    "    key = tuple(modalities)\n",
    "    if key in const_variable.prompt_templates:\n",
    "        question = random.choice(const_variable.prompt_templates[key]) + \". \"\n",
    "    answer = commons.generate_tb_response(modalities, now_row.llm_analyze_symptoms, now_row.llm_analyze_image, positive=(now_row.ground_truth_tb == 1))\n",
    "\n",
    "    if now_row.path_file_audio == 'Unknown':\n",
    "        modalities = [m for m in modalities if m != \"audio\"]\n",
    "\n",
    "    array_df = [None, None]\n",
    "    if \"symptoms\" in modalities:\n",
    "        row_dict = now_row._asdict()\n",
    "        selected_feats = random.sample(const_variable.columns_soundfeat, k=random.randint(3, len(const_variable.columns_soundfeat)))\n",
    "        symptom_descriptions = \", \".join(\n",
    "            f\"{feat.replace('_', ' ')} is {row_dict[feat]}\"\n",
    "            for feat in selected_feats\n",
    "            if row_dict.get(feat) != \"Unknown\"\n",
    "        )\n",
    "        if symptom_descriptions:\n",
    "            question += f\" Also, the patient presents with: {symptom_descriptions}.\"\n",
    "\n",
    "    if \"audio\" in modalities:\n",
    "        array_df[0] = now_audiopath\n",
    "\n",
    "    if \"xray\" in modalities:\n",
    "        array_df[1] = now_imgaepath\n",
    "        xray_descriptions = \", \".join(\n",
    "            f\"{feat.replace('_', ' ')} is {row_dict[feat]}\"\n",
    "            for feat in const_variable.columns_imagefeat\n",
    "            if row_dict.get(feat) != \"Unknown\"\n",
    "        )\n",
    "        if xray_descriptions:\n",
    "            question += f\" Additional chest x-ray metadata include: {xray_descriptions}.\"\n",
    "\n",
    "    question = question.strip()\n",
    "    question = question.rstrip(\",.\")\n",
    "    if not question.endswith(\".\"):\n",
    "        question += \".\"\n",
    "\n",
    "    temp_instruct = {\"messages\": [\n",
    "        {\"role\": \"system\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": system_prompt}\n",
    "            ],\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": question},\n",
    "            ],\n",
    "        },\n",
    "        {\"role\": \"assistant\", \"content\": [\n",
    "            {\"type\": \"text\", \"text\": answer}]},\n",
    "    ]}\n",
    "    if array_df[0] != None:\n",
    "        temp_instruct[\"messages\"][1]['content'].append({\"type\": \"audio\", \"audio\": array_df[0]})\n",
    "    if array_df[1] != None:\n",
    "        temp_instruct[\"messages\"][1]['content'].append({\"type\": \"image\", \"image\":array_df[1]})\n",
    "\n",
    "    if now_row.ground_truth_tb == 1:\n",
    "        instruct_array_positive.append(temp_instruct)\n",
    "    elif now_row.ground_truth_tb == 0:\n",
    "        instruct_array_negative.append(temp_instruct)\n",
    "        \n",
    "with open(f\"datas/positive_instruct.pkl.test\", \"wb\") as f:\n",
    "    pickle.dump(instruct_array_positive, f)\n",
    "\n",
    "with open(f\"datas/negative_instruct.pkl.test\", \"wb\") as f:\n",
    "    pickle.dump(instruct_array_negative, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "759dc5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_memory():\n",
    "    # Delete variables if they exist in the current global scope\n",
    "    if \"inputs\" in globals():\n",
    "        del globals()[\"inputs\"]\n",
    "    if \"model\" in globals():\n",
    "        del globals()[\"model\"]\n",
    "    if \"processor\" in globals():\n",
    "        del globals()[\"processor\"]\n",
    "    if \"trainer\" in globals():\n",
    "        del globals()[\"trainer\"]\n",
    "    if \"peft_model\" in globals():\n",
    "        del globals()[\"peft_model\"]\n",
    "    if \"bnb_config\" in globals():\n",
    "        del globals()[\"bnb_config\"]\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Garbage collection and clearing CUDA memory\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.synchronize()\n",
    "    time.sleep(2)\n",
    "    gc.collect()\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(f\"GPU allocated memory: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU reserved memory: {torch.cuda.memory_reserved() / 1024**3:.2f} GB\")\n",
    "\n",
    "\n",
    "clear_memory()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
