{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "657bdbd7",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1b132e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/is/dwipraseetyo-a/NAS_HAI/Project/Qwen2.5-Omni\n",
    "%pwd\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "import transformers, torch\n",
    "device_name = torch.cuda.get_device_name(0)\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    props = torch.cuda.get_device_properties(i)\n",
    "    print(f\"Logical index: {i}, Name: {props.name}\")\n",
    "\n",
    "import commons, const_variable\n",
    "import os, librosa, random, pickle, pydicom, requests, torch, re, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c282a37",
   "metadata": {},
   "source": [
    "# Iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49c9ed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = '''\n",
    "You are a clinical reasoning assistant specialized in respiratory diseases, particularly Tuberculosis (TB).\n",
    "The user will provide a label (True for TB, False for not TB) along with a list of symptoms. Treat the label as ground truth.\n",
    "Your task is to perform detailed clinical reasoning by analyzing the provided symptoms and explaining why they support or contradict the TB diagnosis.\n",
    "Output format: JSONL with one key 'answer'\n",
    "The value should be a short paragraph of clinical reasoning, grounded in the symptoms.\n",
    "'''\n",
    "DATA_PATH = \"/home/is/dwipraseetyo-a/NAS_HAI/Datasets/cidrz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33aa1594",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train', 'dev', 'test']:\n",
    "    df = pd.read_csv(f\"{DATA_PATH}/metadata_cut_processed.csv.{split}\")\n",
    "    df = df.rename(columns={'coughdur': 'cough_duration', 'ngtsweats': 'night_sweets', 'weightloss': 'weight_loss', 'body_wt': 'body_weight'})\n",
    "\n",
    "    # old_barcodes = set()\n",
    "    # for split in ['train', 'dev', 'test']:\n",
    "    #     old_file = f\"datas/reasoning/symptoms/gpt41_symptoms.csv.{split}\"\n",
    "    #     old_df = pd.read_csv(old_file)\n",
    "    #     old_barcodes.update(old_df['barcode'].unique())\n",
    "\n",
    "    # missing_df = df[~df['barcode'].isin(old_barcodes)].copy()\n",
    "\n",
    "    df_temp = pd.DataFrame(columns=['barcode', 'llm_analyze_symptoms'])\n",
    "    for now_row in tqdm(df.itertuples(), desc=f\"Processing {split}\", total=len(df)):\n",
    "        row_dict = now_row._asdict()\n",
    "        now_barcode = now_row.barcode\n",
    "        answer = \"This is Tuberculosis Positive\" if now_row.ground_truth_tb == 1 else \"This is Tuberculosis Negative\"\n",
    "        question = \", \".join(\n",
    "            f\"{feat.replace('_', ' ')} is {row_dict[feat]}\"\n",
    "            for feat in const_variable.columns_soundfeat\n",
    "            if row_dict.get(feat) != \"Unknown\"\n",
    "        )\n",
    "\n",
    "        response = const_variable.clientOpenAI.chat.completions.create(\n",
    "            model=\"gpt-4o-mini\",  # o4-mini gpt-4o-mini gpt-4.1\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"{answer}. The Patient Symptoms Are: {question}\"}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        safe_content = re.sub(r'(?<!\\\\)\\n', r'\\\\n', response.choices[0].message.content.replace(\"{\\n\", \"{\").replace(\"\\n}\", \"}\"))\n",
    "        if not safe_content.endswith('}'):\n",
    "            safe_content += '}'\n",
    "        response_json = json.loads(safe_content)\n",
    "        llm_answer = response_json['answer']\n",
    "        df_temp.loc[len(df_temp)] = [now_barcode, llm_answer]\n",
    "\n",
    "    pd.DataFrame(df_temp).to_csv(f'datas/reasoning/symptoms/gpt-4o-mini_symptoms.csv.{split}', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd12cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for now_row in tqdm(df.itertuples(), desc=f\"Processing {split}\", total=len(df)):\n",
    "    count += 1\n",
    "    if count <= 620:\n",
    "        continue\n",
    "    \n",
    "    row_dict = now_row._asdict()\n",
    "    now_barcode = now_row.barcode\n",
    "    answer = \"This is Tuberculosis\" if now_row.ground_truth_tb == 1 else \"This is Not Tuberculosis\"\n",
    "    question = \", \".join(\n",
    "        f\"{feat.replace('_', ' ')} is {row_dict[feat]}\"\n",
    "        for feat in const_variable.columns_soundfeat\n",
    "        if row_dict.get(feat) != \"Unknown\"\n",
    "    )\n",
    "\n",
    "    response = const_variable.clientOpenAI.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",  # o4-mini gpt-4o-mini\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"{answer}. The Patient Symptoms Are: {question}\"}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    safe_content = re.sub(r'(?<!\\\\)\\n', r'\\\\n', response.choices[0].message.content.replace(\"{\\n\", \"{\").replace(\"\\n}\", \"}\"))\n",
    "    if not safe_content.endswith('}'):\n",
    "        safe_content += '}'\n",
    "    response_json = json.loads(safe_content)\n",
    "    llm_answer = response_json['answer']\n",
    "    df_temp.loc[len(df_temp)] = [now_barcode, llm_answer]\n",
    "pd.DataFrame(df_temp).to_csv(f'datas/reasoning/symptoms/gpt4_symptoms.csv.{split}', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b704b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "old_dfs = []\n",
    "for split in ['train', 'dev', 'test']:\n",
    "    old_file = f\"datas/reasoning/symptoms/gpt41_symptoms.csv.{split}\"\n",
    "    if os.path.exists(old_file):\n",
    "        old_df = pd.read_csv(old_file)\n",
    "        old_dfs.append(old_df)\n",
    "\n",
    "if old_dfs:\n",
    "    old_all = pd.concat(old_dfs, ignore_index=True)\n",
    "else:\n",
    "    old_all = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e3c907",
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['train', 'dev', 'test']:\n",
    "    df = pd.read_csv(f\"{DATA_PATH}/metadata_cut_processed.csv.{split}\")\n",
    "    df = df.rename(columns={'coughdur': 'cough_duration', 'ngtsweats': 'night_sweets', 'weightloss': 'weight_loss', 'body_wt': 'body_weight'})\n",
    "\n",
    "    df_temp = pd.DataFrame(columns=['barcode', 'llm_analyze_symptoms'])\n",
    "    for now_row in tqdm(df.itertuples(), desc=f\"Processing {split}\", total=len(df)):\n",
    "        row_dict = now_row._asdict()\n",
    "        now_barcode = now_row.barcode\n",
    "        \n",
    "        selected_row = old_all[old_all['barcode'] == now_barcode]\n",
    "        llm_answer = selected_row.sample(n=1).iloc[0]['llm_analyze_symptoms']\n",
    "        df_temp.loc[len(df_temp)] = [now_barcode, llm_answer]\n",
    "\n",
    "    pd.DataFrame(df_temp).to_csv(f'datas/reasoning/symptoms/gpt-4o-mini_symptoms.csv.{split}', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b77e777",
   "metadata": {},
   "source": [
    "# OpenBioLLM-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64509a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/is/dwipraseetyo-a/NAS_HAI/Project/Qwen2.5-Omni\n",
    "%pwd\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "print(\"CUDA_VISIBLE_DEVICES:\", os.environ[\"CUDA_VISIBLE_DEVICES\"])\n",
    "\n",
    "import transformers, torch\n",
    "device_name = torch.cuda.get_device_name(0)\n",
    "for i in range(torch.cuda.device_count()):\n",
    "    props = torch.cuda.get_device_properties(i)\n",
    "    print(f\"Logical index: {i}, Name: {props.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cf701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import commons, const_variable\n",
    "import os, librosa, random, pickle, pydicom, requests, torch, re, json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_PATH = \"/home/is/dwipraseetyo-a/NAS_HAI/Datasets/cidrz\"\n",
    "model_id = \"/home/is/dwipraseetyo-a/NAS_HAI/Project/pretrain/Llama3-OpenBioLLM-8B\"\n",
    "system_prompt = \"You are an expert and experienced from the healthcare and biomedical domain with extensive medical knowledge and practical experience. Your name is OpenBioLLM, and you were developed by Saama AI Labs. who's willing to help answer the user's query with explanation. In your explanation, leverage your deep medical expertise such as relevant anatomical structures, physiological processes, diagnostic criteria, treatment guidelines, or other pertinent medical concepts. Use precise medical terminology while still aiming to make the explanation clear and accessible to a general audience.\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=\"bfloat16\",\n",
    "    device_map=\"auto\" \n",
    ")\n",
    "\n",
    "pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6766cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Turberculosis\n",
    "Pneumonia\n",
    "Covid19\n",
    "Healthy\n",
    "Others\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
