{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874aafaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Global seed set to 42\n",
      "[INFO:swift] output_dir: /home/ldap-users-2/dwipraseetyo-a/Project/Qwen2.5-Omni/outputs/qwen25omni3b-think-balance-grpo\n",
      "[INFO:swift] Downloading the model from ModelScope Hub, model_id: Qwen/Qwen2.5-Omni-3B\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ldap-users-2/dwipraseetyo-a/Project/Qwen2.5-Omni\n",
      "Downloading Model from https://www.modelscope.cn to directory: /home/is/dwipraseetyo-a/.cache/modelscope/hub/models/Qwen/Qwen2.5-Omni-3B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:modelscope] Target directory already exists, skipping creation.\n",
      "[INFO:swift] Loading the model using model_dir: /home/is/dwipraseetyo-a/.cache/modelscope/hub/models/Qwen/Qwen2___5-Omni-3B\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "[INFO:swift] Setting torch_dtype: torch.bfloat16\n",
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n",
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n",
      "[INFO:swift] model_kwargs: {'device_map': 'auto'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "add210bf97c74911a3dc272900093819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] default_system: 'A conversation between User and Advanced medical assistant specialized in analyzing and diagnosing clinical conditions. and the Assistant determines whether the case is Positive or Negative. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer>'\n",
      "[INFO:swift] max_length: 2048\n",
      "[INFO:swift] response_prefix: ''\n",
      "[INFO:swift] agent_template: hermes\n",
      "[INFO:swift] norm_bbox: none\n",
      "[INFO:swift] lora_config: LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/home/is/dwipraseetyo-a/.cache/modelscope/hub/models/Qwen/Qwen2___5-Omni-3B', revision=None, inference_mode=False, r=8, target_modules={'q_proj', 'up_proj', 'k', 'fc2', 'k_proj', 'attn.q', 'down_proj', 'audio_tower.proj', 'v', 'attn.proj', 'v_proj', 'out_proj', 'fc1', 'gate_proj', 'mlp.2', 'mlp.0', 'o_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, use_qalora=False, qalora_group_size=16, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, lora_dtype=None, lorap_lr_ratio=None, lorap_emb_lr=1e-06)\n",
      "[INFO:swift] model_parameter_info: PeftModelForCausalLM: 4730.7264M Params (27.2620M Trainable [0.5763%]), 1.9224M Buffers.\n"
     ]
    }
   ],
   "source": [
    "%cd /home/is/dwipraseetyo-a/NAS_HAI/Project/Qwen2.5-Omni\n",
    "import pickle, os, re, random, torch\n",
    "from swift.llm import get_model_tokenizer, load_dataset, get_template, EncodePreprocessor\n",
    "from swift.utils import get_logger, find_all_linears, get_model_parameter_info, plot_images, seed_everything\n",
    "from swift.tuners import Swift, LoraConfig\n",
    "from swift.trainers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "from datasets import Dataset\n",
    "from functools import partial\n",
    "import commons\n",
    "import const_variable\n",
    "import logging, warnings\n",
    "class SuppressMultipleWarnings(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        suppressed_msgs = [\n",
    "            \"Trainer.tokenizer is now deprecated\",\n",
    "            \"System prompt modified, audio output may not work as expected\"\n",
    "        ]\n",
    "        return not any(record.getMessage().startswith(msg) for msg in suppressed_msgs)\n",
    "logging.getLogger().addFilter(SuppressMultipleWarnings())\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "logger = get_logger()\n",
    "seed_everything(42)\n",
    "\n",
    "# Hyperparameters for training\n",
    "os.environ.update({\n",
    "    \"MAX_PIXELS\": \"1003520\",\n",
    "    \"NPROC_PER_NODE\": \"4\",\n",
    "    \"ENABLE_AUDIO_OUTPUT\": \"0\",\n",
    "    \"CUDA_VISIBLE_DEVICES\": \"0,1\"\n",
    "})\n",
    "\n",
    "model_id_or_path = 'Qwen/Qwen2.5-Omni-3B'\n",
    "output_dir = 'outputs/qwen25omni3b-think-balance-grpo'\n",
    "\n",
    "data_seed = 42\n",
    "max_length = 2048\n",
    "split_dataset_ratio = 0.01  # Split validation set\n",
    "num_proc = 4  # The number of processes for data loading.\n",
    "\n",
    "# lora\n",
    "lora_rank = 8\n",
    "lora_alpha = 32\n",
    "\n",
    "# training_args\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_checkpointing=True,\n",
    "    weight_decay=0.1,\n",
    "    lr_scheduler_type='cosine',\n",
    "    warmup_ratio=0.05,\n",
    "    report_to=['tensorboard'],\n",
    "    logging_first_step=True,\n",
    "    save_strategy='steps',\n",
    "    save_steps=50,\n",
    "    eval_strategy='steps',\n",
    "    eval_steps=50,\n",
    "    gradient_accumulation_steps=16,\n",
    "    num_train_epochs=1,\n",
    "    metric_for_best_model='loss',\n",
    "    save_total_limit=2,\n",
    "    logging_steps=5,\n",
    "    dataloader_num_workers=1,\n",
    "    data_seed=data_seed,\n",
    ")\n",
    "\n",
    "output_dir = os.path.abspath(os.path.expanduser(output_dir))\n",
    "logger.info(f'output_dir: {output_dir}')\n",
    "\n",
    "# Obtain the model and template, and add a trainable Lora layer on the model.\n",
    "model, tokenizer = get_model_tokenizer(model_id_or_path)\n",
    "template = get_template(model.model_meta.template, tokenizer, default_system=const_variable.system_prompt, max_length=max_length)\n",
    "template.set_mode('train')\n",
    "if template.use_model:\n",
    "    template.model = model\n",
    "\n",
    "target_modules = find_all_linears(model)\n",
    "lora_config = LoraConfig(task_type='CAUSAL_LM', r=lora_rank, lora_alpha=lora_alpha,\n",
    "                         target_modules=target_modules)\n",
    "model = Swift.prepare_model(model, lora_config)\n",
    "logger.info(f'lora_config: {lora_config}')\n",
    "\n",
    "# Print model structure and trainable parameters.\n",
    "model_parameter_info = get_model_parameter_info(model)\n",
    "logger.info(f'model_parameter_info: {model_parameter_info}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "be5ffb47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Downloading the dataset from ModelScope, dataset_id: AI-ModelScope/LaTeX_OCR\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset = load_dataset(['AI-ModelScope/LaTeX_OCR#25'], split_dataset_ratio=split_dataset_ratio, num_proc=num_proc,\n",
    "                                          seed=data_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f42b5755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================\n",
      "| 📦 Loading Dataset... |\n",
      "========================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                                                                                 | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 72.07it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 118.34it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 106.73it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 104.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import commons\n",
    "import const_variable\n",
    "import copy\n",
    "from qwen_omni_utils import process_mm_info\n",
    "import io\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "def numpy_to_wav_bytes(audio_array: np.ndarray, sr: int) -> bytes:\n",
    "    \"\"\"\n",
    "    Convert a NumPy array to WAV bytes in memory.\n",
    "    \"\"\"\n",
    "    buf = io.BytesIO()\n",
    "    sf.write(buf, audio_array, sr, format='WAV')\n",
    "    return buf.getvalue()\n",
    "\n",
    "def grpo_build_datasets(instruct, processor):\n",
    "    datasets = []\n",
    "    for sample in tqdm(instruct):\n",
    "        image_found = False\n",
    "        audio_found = False\n",
    "        conversation  = copy.deepcopy(sample[\"messages\"])\n",
    "        for ele in conversation[1]['content']:\n",
    "            if ele[\"type\"] == \"audio\":\n",
    "                if \"audio\" in ele or \"audio_url\" in ele:\n",
    "                    path = ele.get(\"audio\", ele.get(\"audio_url\"))\n",
    "                    start_sec, end_sec = commons.random_3sec_segment(path, segment_duration=3.0)\n",
    "                    ele[\"audio_start\"] = float(start_sec)\n",
    "                    ele[\"audio_end\"] = float(end_sec)\n",
    "                    audio_found = True\n",
    "            if ele[\"type\"] == \"image\":\n",
    "                if \"image\" in ele or \"audio_url\" in ele:\n",
    "                    image_found = True\n",
    "\n",
    "        audios, images, videos = process_mm_info(conversation, use_audio_in_video=False)\n",
    "        converted = [\n",
    "            {\n",
    "                \"role\": item[\"role\"],\n",
    "                \"content\": next(\n",
    "                    (ele[\"text\"] for ele in item[\"content\"] if ele[\"type\"] not in (\"audio\", \"image\")),\n",
    "                    \"\"\n",
    "                )\n",
    "            }\n",
    "            for item in conversation\n",
    "        ]\n",
    "\n",
    "        current_object = {'messages': converted}\n",
    "        if images != None:\n",
    "            current_object['images'] = images\n",
    "        if audios != None:\n",
    "            current_object['audios'] = [numpy_to_wav_bytes(audios[0], 16000)]\n",
    "\n",
    "        if image_found == False:\n",
    "            continue\n",
    "        if audio_found == False:\n",
    "            continue \n",
    "\n",
    "        datasets.append(current_object)\n",
    "    return datasets\n",
    "\n",
    "commons.pretty_status(\"📦 Loading Dataset...\")\n",
    "with open('datas/instruct_grpo_balance.pkl.dev', 'rb') as f:\n",
    "    train_instruct = commons.load_image_PIL(pickle.load(f)[0:10])\n",
    "\n",
    "with open('datas/instruct_grpo_balance.pkl.dev', 'rb') as f:\n",
    "    dev_instruct = commons.load_image_PIL(pickle.load(f)[0:10])\n",
    "\n",
    "train_dataset = Dataset.from_list(grpo_build_datasets(train_instruct, tokenizer)) #\n",
    "val_dataset = Dataset.from_list(grpo_build_datasets(train_instruct, tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "49cd2934",
   "metadata": {},
   "outputs": [],
   "source": [
    "from swift.llm import (\n",
    "    get_model_tokenizer, load_dataset, get_template, EncodePreprocessor, get_model_arch,\n",
    "    get_multimodal_target_regex, LazyLLMDataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "260d021f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LazyLLMDataset(train_dataset, template.encode, random_state=data_seed)\n",
    "val_dataset = LazyLLMDataset(val_dataset, template.encode, random_state=data_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155925da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3f9884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05b139f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ldap-users-2/dwipraseetyo-a/Project/Qwen2.5-Omni\n"
     ]
    }
   ],
   "source": [
    "%cd /home/is/dwipraseetyo-a/NAS_HAI/Project/Qwen2.5-Omni\n",
    "import pickle, os, re, random, torch\n",
    "from peft import get_peft_model, LoraConfig, TaskType, get_peft_model_state_dict\n",
    "from qwen_omni_utils import process_mm_info\n",
    "from trl import SFTTrainer, SFTConfig, GRPOConfig, GRPOTrainer\n",
    "from my_qwenwrapper import get_OmniModel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "import commons\n",
    "import const_variable\n",
    "from my_datasets import QwenOmniFinetuneDataset\n",
    "\n",
    "import logging, warnings\n",
    "class SuppressMultipleWarnings(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        suppressed_msgs = [\n",
    "            \"Trainer.tokenizer is now deprecated\",\n",
    "            \"System prompt modified, audio output may not work as expected\"\n",
    "        ]\n",
    "        return not any(record.getMessage().startswith(msg) for msg in suppressed_msgs)\n",
    "logging.getLogger().addFilter(SuppressMultipleWarnings())\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "# warnings.filterwarnings(\"ignore\", message=\"System prompt modified, audio output may not work as expected. Audio output mode only works when using default system prompt 'You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.'\")\n",
    "# warnings.filterwarnings(\"ignore\", message=r\"Trainer\\.tokenizer.*deprecated\")\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a65e9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Processsor.... Using Left padding Side\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The image processor of type `Qwen2VLImageProcessor` is now loaded as a fast processor by default, even if the model checkpoint was saved with a slow processor. This is a breaking change and may produce slightly different outputs. To continue using the slow processor, instantiate this class with `use_fast=False`. Note that this behavior will be extended to all models in a future release.\n",
      "You have video processor config saved in `preprocessor.json` file which is deprecated. Video processor configs should be saved in their own `video_preprocessor.json` file. You can rename the file or load and save the processor back which renames it automatically. Loading from `preprocessor.json` will be removed in v5.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Model.... Using BitsAndBytesConfig\n",
      "Loading Model.... Using Offload Folder\n",
      "Loading Model.... Using Flash Attention\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unrecognized keys in `rope_scaling` for 'rope_type'='default': {'mrope_section'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab0fa90e0ff402abd88cf2f31cb4a13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, processor = get_OmniModel(model_path=\"Qwen/Qwen2.5-Omni-3B\", processor_path=\"Qwen/Qwen2.5-Omni-3B\", padding_side=\"left\",\n",
    "                                use_flash_attention=True, only_processor=False, quantize_4bit=True, \n",
    "                                offload_folder=\"offload\", set_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9d525f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d13611d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 439/439 [00:02<00:00, 153.29it/s]\n"
     ]
    }
   ],
   "source": [
    "with open('datas/instruct_grpo_balance.pkl.dev', 'rb') as f:\n",
    "    dev_instruct = commons.load_image_PIL(pickle.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c703c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "68f4df9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'text',\n",
       "  'text': 'The patient symptoms are body weight is 59.0, shortbreath is no. Given the cough audio and symptoms, is this case Positive Tuberculosis or Negative Tuberculosis?. '},\n",
       " {'type': 'audio', 'audio_start': 2.6, 'audio_end': 5.6}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation[1]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3766e48c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': '<|im_start|>system\\nA conversation between User and Advanced medical assistant specialized in analyzing and diagnosing clinical conditions. and the Assistant determines whether the case is Positive or Negative. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer><|im_end|>\\n<|im_start|>user\\nThe patient symptoms are shortbreath is no, tobacco use is never, height is 152.0, chestpain is no, night sweets is no, fever is no, hiv status is neg, sex is f, cough duration is no cough. The chest x-ray metadata are Modality is CR, ImagerPixelSpacing is [0.175, 0.175], Sensitivity is 250.0, PhotometricInterpretation is MONOCHROME2, Rows is 2446, Columns is 2010, WindowCenter is 2038.0, WindowWidth is 4096.0. Considering all inputs (image, sound, and symptoms), is this Positive Tuberculosis or Negative Tuberculosis?. <|vision_bos|><|IMAGE|><|vision_eos|><|im_end|>\\n<|im_start|>assistant\\n',\n",
       " 'solution': '<think>From the given symptoms, cough sound, and x-ray, Let me Analyze your regrading your questions.\\n\\n*   All modalities are present.\\nThis is a preliminary interpretation based on given data and does not replace a comprehensive clinical evaluation. A definitive diagnosis requires a additional clinical evaluation, including the physical examination findings, Cough Sound, Auscultation Sound, and imaging studies.\\n## 📋 Observations\\n**Symptoms:**\\n*   The symptoms provided indicate that the patient does not exhibit any of the common clinical manifestations associated with Tuberculosis (TB). The absence of cough, chest pain, shortness of breath, fever, night sweats, and weight loss significantly contradicts the typical presentation of TB. Moreover, the negative HIV status and no history of tobacco use further reduce the risk factors commonly associated with TB infection. Overall, the symptoms do not support a TB diagnosis.\\n\\n**Chest X-ray:**\\n* Specific Findings\\nThe lungs are clear bilaterally. The heart size is normal. The mediastinal contours are unremarkable. The bony structures of the chest wall appear intact. No pleural effusion or pneumothorax is identified. \\n\\n* Differential Diagnosis\\nNone apparent. \\n\\nNormal chest X-ray. \\n\\n\\n**Audio:**\\n*   Will be Implemented Soon</think>\\n\\n<answer>Negative Tuberculosis</answer>',\n",
       " 'image': <PIL.Image.Image image mode=L size=1024x1246>}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_datasets[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3ec65a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "A conversation between User and Advanced medical assistant specialized in analyzing and diagnosing clinical conditions. and the Assistant determines whether the case is Positive or Negative. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think><answer> answer here </answer><|im_end|>\n",
      "<|im_start|>user\n",
      "The patient symptoms are body weight is 59.0, shortbreath is no. Given the cough audio and symptoms, is this case Positive Tuberculosis or Negative Tuberculosis?. <|audio_bos|><|AUDIO|><|audio_eos|><|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78289d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>From the given symptoms and cough sound, Let me Analyze your regrading your questions.\n",
      "\n",
      "## ⚠️ Points to Review and Disclaimer\n",
      "* No X-ray Image Provided\n",
      "\n",
      "This is a preliminary interpretation based on given data and does not replace a comprehensive clinical evaluation. A definitive diagnosis requires a additional clinical evaluation, including the physical examination findings, Cough Sound, Auscultation Sound, and imaging studies.\n",
      "## 📋 Observations\n",
      "**Symptoms:**\n",
      "*   The symptoms provided do not support a diagnosis of Tuberculosis (TB). The patient has a productive cough lasting only 1-2 weeks, which is relatively short for TB, as the cough typically persists for months in TB cases. Additionally, the absence of hemoptysis, chest pain, shortness of breath, fever, night sweats, and weight loss further reduces the likelihood of TB, which often presents with these symptoms. The patient's negative HIV status and a healthy body mass index also support the absence of TB, given the higher vulnerability of immunocompromised individuals to this disease. Therefore, these clinical findings lead to the conclusion that this is not TB.\n",
      "\n",
      "**Audio:**\n",
      "*   Will be Implemented Soon</think>\n",
      "\n",
      "<answer>Negative Tuberculosis</answer>\n"
     ]
    }
   ],
   "source": [
    "print(solution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cd3ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multimodal_collate_fn(batch):\n",
    "    prompts = []\n",
    "    for sample in batch:\n",
    "        conversation  = sample[\"messages\"]\n",
    "        for ele in conversation[1]['content']:\n",
    "            if ele[\"type\"] == \"audio\":\n",
    "                if \"audio\" in ele or \"audio_url\" in ele:\n",
    "                    path = ele.get(\"audio\", ele.get(\"audio_url\"))\n",
    "                    start_sec, end_sec = commons.random_3sec_segment(path, segment_duration=3.0)\n",
    "                    ele[\"audio_start\"] = float(start_sec)\n",
    "                    ele[\"audio_end\"] = float(end_sec)\n",
    "        prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n",
    "    return {\n",
    "        \"image\": prompts,\n",
    "        \"image\": prompts,\n",
    "        \"prompts\": prompts,\n",
    "        \"solution\": prompts,\n",
    "    }\n",
    "\n",
    "import re\n",
    "from typing import Optional\n",
    "\n",
    "def format_reward(completions, **kwargs):\n",
    "    \"\"\"Reward function that checks if the completion has a specific format.\"\"\"\n",
    "    pattern = r\"^<think>\\n.*?\\n</think>\\n<answer>\\n.*?\\n</answer>$\"\n",
    "    matches = [re.match(pattern, content, re.DOTALL | re.MULTILINE) for content in completions]\n",
    "    rewards = [1.0 if match else 0.0 for match in matches]\n",
    "    return rewards\n",
    "\n",
    "def accuracy_reward(completions: list[list[dict[str, str]]], solution: list[str], **kwargs) -> list[Optional[float]]:\n",
    "    \"\"\"Reward function that checks if the completion matches the ground truth.\n",
    "    - If both gold and prediction are parseable → use math verification.\n",
    "    - If not parseable → compare as normalized text.\n",
    "    \"\"\"\n",
    "    rewards = []\n",
    "    for completion, sol in zip(completions, solution):\n",
    "        reward = float(completion.strip().lower() == sol.strip().lower())\n",
    "        rewards.append(reward)\n",
    "    return rewards\n",
    "\n",
    "##########################################################################################################\n",
    "commons.pretty_status(\"🧠 Loading Model...\")\n",
    "\n",
    "peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, \n",
    "                        inference_mode=False, \n",
    "                        r=8, \n",
    "                        lora_alpha=32, \n",
    "                        lora_dropout=0.05, \n",
    "                        target_modules=[\"q_proj\", \"v_proj\"])\n",
    "                        #target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"])\n",
    "\n",
    "model, processor = get_OmniModel(model_path=\"Qwen/Qwen2.5-Omni-3B\", processor_path=\"Qwen/Qwen2.5-Omni-3B\", padding_side=\"left\",\n",
    "                                use_flash_attention=True, only_processor=False, quantize_4bit=True, \n",
    "                                offload_folder=\"offload\", set_eval=False)\n",
    "\n",
    "###\n",
    "# How about we finetuning the audio and image encoder, not using PEFT, or increase PEFT to audio and image encoder\n",
    "\n",
    "#model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "peft_model = get_peft_model(model, peft_config)\n",
    "peft_model.print_trainable_parameters()\n",
    "model = peft_model.unload()\n",
    "del peft_model\n",
    "\n",
    "##########################################################################################################\n",
    "commons.pretty_status(\"📦 Loading Dataset...\")\n",
    "with open('datas/instruct_grpo_balance.pkl.train', 'rb') as f:\n",
    "    train_instruct = commons.load_image_PIL(pickle.load(f))\n",
    "\n",
    "with open('datas/instruct_grpo_balance.pkl.dev', 'rb') as f:\n",
    "    dev_instruct = commons.load_image_PIL(pickle.load(f))\n",
    "\n",
    "train_dataset = QwenOmniFinetuneDataset(train_instruct, processor, use_audio_in_video=False)\n",
    "dev_dataset = QwenOmniFinetuneDataset(dev_instruct, processor, use_audio_in_video=False)\n",
    "print(train_dataset[0])\n",
    "dataset = Dataset.from_list(instruct_array)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
